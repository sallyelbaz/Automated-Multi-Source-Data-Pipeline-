# -*- coding: utf-8 -*-
"""Multi Source Data Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PqkRg_Zd6grQsxz3Uq1DY2opkK9F7azM

##Automated Multi-Source Data Pipeline

Problem: Data is fragmented across many internal systems. Analysts manually pull data, causing
delays and errors.

Solution: Build an automated pipeline that connects to multiple data sources (APIs, SQL database, flat
files), cleans the data, and loads it into a cloud warehouse (BigQuery/Snowflake). Set up scheduled
automation.

Tech Stack: Python, SQL, Airflow, dbt, BigQuery/Snowflake, GitHub Actions
Expected Business Impact: Reduced manual reporting effort by 90%. Enabled real time
decision making.
"""



# loading the necessary libraries
import pandas as pd
import numpy as np
import os
import json
from google.cloud import bigquery
from google.oauth2 import service_account
from pandas_gbq import to_gbq


# Load the JSON from Github Actions Secret
key_dict = json.loads(os.environ['BIGQUERY_CREDENTIALS_JSON'])
credentials = service_account.Credentials.from_service_account_info(key_dict)

# loading the customer data
cust_url = "https://raw.githubusercontent.com/sallyelbaz/Automated-Multi-Source-Data-Pipeline-/refs/heads/main/data/customers.csv"
cust = pd.read_csv(cust_url)
cust.head()

# Standardizing Customer Data
# renaming the columns
cust = cust.rename(columns={
    "customer_id": "CustID",
    "first_name": "First Name",
    "last_name": "Last Name",
    "country":"Country",
    "signup_date" : "Sign Up Date"
})
# Converting data types
cust = cust.astype({
    "First Name": "str",
    "Last Name" : "str",
    "Country" : "str",
    "Sign Up Date":"datetime64[ns]"
})

# loading the orders data
orders_url = "https://raw.githubusercontent.com/sallyelbaz/Automated-Multi-Source-Data-Pipeline-/refs/heads/main/data/orders.csv"
orders = pd.read_csv(orders_url)
orders.head()

# Standardizing Order Data
# Changing Column Names
orders = orders.rename(columns={
  "order_id": "OrderID",
  "customer_id": "CustID",
  "order_amount": "Order Amount",
  "order_date" : "Order Date"
})
# Changing Data Types
orders = orders.astype({
    "Order Date":"datetime64[ns]"
})
print(orders.dtypes)

# Merging the data together into one table
cust_orders = pd.merge(orders,cust,on="CustID")
cust_orders.head()

# Setting the BigQuery Parameters
project_id = "prime-hour-477815-g3"
dataset_id = "CustOrders"
table_id = "CustOrders"
destination_table = f"{project_id}.{dataset_id}.{table_id}"

to_gbq(
    cust_orders,
    destination_table=destination_table,
    project_id=project_id,
    if_exists='replace',   # replace existing table
    credentials=credentials
)
